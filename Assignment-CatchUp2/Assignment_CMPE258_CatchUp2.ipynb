{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-CMPE258-CatchUp2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Write colab to demonstrate in graph neural networks using pytorch geometric using both GCN and GAT layers.\n",
        " \n",
        "\n",
        "a) Graph Classification Problem\n",
        "\n",
        "b) Edge Prediction Problem\n",
        "\n",
        "c) Node Classification Problem"
      ],
      "metadata": {
        "id": "bqkCAGVozJgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction about Graph Neural Networks (GNN): Graph Neural Networks (GNNs) have recently gained increasing popularity in both applications and research, including domains such as social networks, knowledge graphs, recommender systems, and bioinformatics. While the theory and math behind GNNs might first seem complicated, the implementation of those models is quite simple and helps in understanding the methodology. Therefore, we will discuss the implementation of basic network layers of a GNN, namely graph convolutions, and attention layers. Finally, we will apply a GNN on a node-level, edge-level, and graph-level tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "RCAG-kKh2m5U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIIVYLBSzAjz",
        "outputId": "5dcbf7a1-287d-4a3d-b7a0-698993f0cb7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np \n",
        "import time\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial7\"\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"NodeLevelMLP.ckpt\", \"NodeLevelGNN.ckpt\", \"GraphLevelGraphConv.ckpt\"]\n",
        "\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if \"/\" in file_name:\n",
        "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JqiWkE6zT88",
        "outputId": "e4440b01-8abd-4351-842d-c504f857735c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/NodeLevelMLP.ckpt...\n",
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/NodeLevelGNN.ckpt...\n",
            "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial7/GraphLevelGraphConv.ckpt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When implementing the GCN layer in PyTorch, we can take advantage of the flexible operations on tensors. Instead of defining a matrix \n",
        ", we can simply divide the summed messages by the number of neighbors afterward. Additionally, we replace the weight matrix with a linear layer, which additionally allows us to add a bias. Written as a PyTorch module, the GCN layer is defined as follows:"
      ],
      "metadata": {
        "id": "s0jlPY5T0nNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_out):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(c_in, c_out)\n",
        "\n",
        "    def forward(self, node_feats, adj_matrix):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Tensor with node features of shape [batch_size, num_nodes, c_in]\n",
        "            adj_matrix - Batch of adjacency matrices of the graph. If there is an edge from i to j, adj_matrix[b,i,j]=1 else 0.\n",
        "                         Supports directed edges by non-symmetric matrices. Assumes to already have added the identity connections. \n",
        "                         Shape: [batch_size, num_nodes, num_nodes]\n",
        "        \"\"\"\n",
        "        # Num neighbours = number of incoming edges\n",
        "        num_neighbours = adj_matrix.sum(dim=-1, keepdims=True)\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = torch.bmm(adj_matrix, node_feats)\n",
        "        node_feats = node_feats / num_neighbours\n",
        "        return node_feats\n"
      ],
      "metadata": {
        "id": "3FaTzApczXLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand GCN layer, we can apply it to our example graph above. specify some node features and the adjacency matrix with added self-connections:"
      ],
      "metadata": {
        "id": "UJB0nYN60r8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_feats = torch.arange(8, dtype=torch.float32).view(1, 4, 2)\n",
        "adj_matrix = torch.Tensor([[[1, 1, 0, 0],\n",
        "                            [1, 1, 1, 1],\n",
        "                            [0, 1, 1, 1],\n",
        "                            [0, 1, 1, 1]]])\n",
        "\n",
        "print(\"Node features:\\n\", node_feats)\n",
        "print(\"\\nAdjacency matrix:\\n\", adj_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3FFDcjKzZiz",
        "outputId": "ef846541-d9fb-4275-dddd-1e84d6958a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node features:\n",
            " tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "\n",
            "Adjacency matrix:\n",
            " tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To GCN layer initialize the linear weight matrix as an identity matrix so that the input features are equal to the messages. This makes it easier for us to verify the message passing operation."
      ],
      "metadata": {
        "id": "-9IETLXM01pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = GCNLayer(c_in=2, c_out=2)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg5rnF7JzbwR",
        "outputId": "00e7417d-f7b0-4445-a469-7f45d66945e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1., 2.],\n",
            "         [3., 4.],\n",
            "         [4., 5.],\n",
            "         [4., 5.]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph Attention.    \n",
        "If you remember from the last tutorial, attention describes a weighted average of multiple elements with the weights dynamically computed based on an input query and elements' keys. This concept can be similarly applied to graphs, one of such is the Graph Attention Network called GAT"
      ],
      "metadata": {
        "id": "WNG-NXjX1M5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GATLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_out, num_heads=1, concat_heads=True, alpha=0.2):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimensionality of input features\n",
        "            c_out - Dimensionality of output features\n",
        "            num_heads - Number of heads, i.e. attention mechanisms to apply in parallel. The \n",
        "                        output features are equally split up over the heads if concat_heads=True.\n",
        "            concat_heads - If True, the output of the different heads is concatenated instead of averaged.\n",
        "            alpha - Negative slope of the LeakyReLU activation.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.concat_heads = concat_heads\n",
        "        if self.concat_heads:\n",
        "            assert c_out % num_heads == 0, \"Number of output features must be a multiple of the count of heads.\"\n",
        "            c_out = c_out // num_heads\n",
        "        \n",
        "        # Sub-modules and parameters needed in the layer\n",
        "        self.projection = nn.Linear(c_in, c_out * num_heads)\n",
        "        self.a = nn.Parameter(torch.Tensor(num_heads, 2 * c_out)) # One per head\n",
        "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
        "        \n",
        "        # Initialization from the original implementation\n",
        "        nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        \n",
        "    def forward(self, node_feats, adj_matrix, print_attn_probs=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            node_feats - Input features of the node. Shape: [batch_size, c_in]\n",
        "            adj_matrix - Adjacency matrix including self-connections. Shape: [batch_size, num_nodes, num_nodes]\n",
        "            print_attn_probs - If True, the attention weights are printed during the forward pass (for debugging purposes)\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
        "        \n",
        "        # Apply linear layer and sort nodes by head\n",
        "        node_feats = self.projection(node_feats)\n",
        "        node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
        "        \n",
        "        # We need to calculate the attention logits for every edge in the adjacency matrix \n",
        "        # Doing this on all possible combinations of nodes is very expensive\n",
        "        # => Create a tensor of [W*h_i||W*h_j] with i and j being the indices of all edges\n",
        "        edges = adj_matrix.nonzero(as_tuple=False) # Returns indices where the adjacency matrix is not 0 => edges\n",
        "        node_feats_flat = node_feats.view(batch_size * num_nodes, self.num_heads, -1)\n",
        "        edge_indices_row = edges[:,0] * num_nodes + edges[:,1]\n",
        "        edge_indices_col = edges[:,0] * num_nodes + edges[:,2]\n",
        "        a_input = torch.cat([\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_row, dim=0),\n",
        "            torch.index_select(input=node_feats_flat, index=edge_indices_col, dim=0)\n",
        "        ], dim=-1) # Index select returns a tensor with node_feats_flat being indexed at the desired positions along dim=0\n",
        "        \n",
        "        # Calculate attention MLP output (independent for each head)\n",
        "        attn_logits = torch.einsum('bhc,hc->bh', a_input, self.a) \n",
        "        attn_logits = self.leakyrelu(attn_logits)\n",
        "        \n",
        "        # Map list of attention values back into a matrix\n",
        "        attn_matrix = attn_logits.new_zeros(adj_matrix.shape+(self.num_heads,)).fill_(-9e15)\n",
        "        attn_matrix[adj_matrix[...,None].repeat(1,1,1,self.num_heads) == 1] = attn_logits.reshape(-1)\n",
        "        \n",
        "        # Weighted average of attention\n",
        "        attn_probs = F.softmax(attn_matrix, dim=2)\n",
        "        if print_attn_probs:\n",
        "            print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
        "        node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
        "        \n",
        "        # If heads should be concatenated, we can do this by reshaping. Otherwise, take mean\n",
        "        if self.concat_heads:\n",
        "            node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
        "        else:\n",
        "            node_feats = node_feats.mean(dim=2)\n",
        "        \n",
        "        return node_feats\n"
      ],
      "metadata": {
        "id": "e8hM-xD5zePI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply the graph attention layer on our example graph above to understand the dynamics better. As before, the input layer is initialized as an identity matrix, but we set  𝐚  to be a vector of arbitrary numbers to obtain different attention values. We use two heads to show the parallel, independent attention mechanisms working in the layer."
      ],
      "metadata": {
        "id": "ywV112-k1YKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = GATLayer(2, 2, num_heads=2)\n",
        "layer.projection.weight.data = torch.Tensor([[1., 0.], [0., 1.]])\n",
        "layer.projection.bias.data = torch.Tensor([0., 0.])\n",
        "layer.a.data = torch.Tensor([[-0.2, 0.3], [0.1, -0.1]])\n",
        "\n",
        "with torch.no_grad():\n",
        "    out_feats = layer(node_feats, adj_matrix, print_attn_probs=True)\n",
        "\n",
        "print(\"Adjacency matrix\", adj_matrix)\n",
        "print(\"Input features\", node_feats)\n",
        "print(\"Output features\", out_feats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrNxfN7xzkow",
        "outputId": "ba50b808-24f0-468f-ddc6-ae06d8f20889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention probs\n",
            " tensor([[[[0.3543, 0.6457, 0.0000, 0.0000],\n",
            "          [0.1096, 0.1450, 0.2642, 0.4813],\n",
            "          [0.0000, 0.1858, 0.2885, 0.5257],\n",
            "          [0.0000, 0.2391, 0.2696, 0.4913]],\n",
            "\n",
            "         [[0.5100, 0.4900, 0.0000, 0.0000],\n",
            "          [0.2975, 0.2436, 0.2340, 0.2249],\n",
            "          [0.0000, 0.3838, 0.3142, 0.3019],\n",
            "          [0.0000, 0.4018, 0.3289, 0.2693]]]])\n",
            "Adjacency matrix tensor([[[1., 1., 0., 0.],\n",
            "         [1., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.],\n",
            "         [0., 1., 1., 1.]]])\n",
            "Input features tensor([[[0., 1.],\n",
            "         [2., 3.],\n",
            "         [4., 5.],\n",
            "         [6., 7.]]])\n",
            "Output features tensor([[[1.2913, 1.9800],\n",
            "         [4.2344, 3.7725],\n",
            "         [4.6798, 4.8362],\n",
            "         [4.5043, 4.7351]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Geometric.    \n",
        "To implement graph networks with adjacency matrix is simple and straight-forward but can be computationally expensive for large graphs. Many real-world graphs can reach over 200k nodes, for which adjacency matrix-based implementations fail. There are a lot of optimizations possible when implementing GNNs, and luckily, there exist packages that provide such layers. The most popular packages for PyTorch are PyTorch Geometric and the Deep Graph Library "
      ],
      "metadata": {
        "id": "_7vlXD3QzpIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch geometric\n",
        "try: \n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Installing torch geometric packages with specific CUDA+PyTorch version. \n",
        "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details \n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "\n",
        "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-geometric \n",
        "    import torch_geometric\n",
        "import torch_geometric.nn as geom_nn\n",
        "import torch_geometric.data as geom_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDR9rQtkzm1X",
        "outputId": "c5649e14-938a-446a-f9bf-f64ec668b8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 15.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 8.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 9.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 16.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=e96d65e843e12865450598055ebed2dbae49de367252495706f770d37fae0a8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNN Layer.     \n",
        "PyTorch Geometric provides us a set of common graph layers, including the GCN and GAT layer "
      ],
      "metadata": {
        "id": "OJAoq-nBzujd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gnn_layer_by_name = {\n",
        "    \"GCN\": geom_nn.GCNConv,\n",
        "    \"GAT\": geom_nn.GATConv,\n",
        "    \"GraphConv\": geom_nn.GraphConv\n",
        "}"
      ],
      "metadata": {
        "id": "f8G3ICCazr-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Node Classification Problem\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oyB2H1g4z1TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cora_dataset = torch_geometric.datasets.Planetoid(root=DATASET_PATH, name=\"Cora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH3X5AsxzwcP",
        "outputId": "f116be3c-2095-49d4-8ead-7d6dd866ef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch Geometric represents the graph data. Note that although we have a single graph, PyTorch Geometric returns a dataset for compatibility to other datasets."
      ],
      "metadata": {
        "id": "a-I2LsRy17au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cora_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07DeEK_Jz7z-",
        "outputId": "0a950ee6-1236-41e0-cd5f-ec712e132440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNN Model"
      ],
      "metadata": {
        "id": "lVZl13D50CDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The edge index tensor is the list of edges in the graph and contains the mirrored version of each edge for undirected graphs. The train_mask, val_mask, and test_mask are boolean masks that indicate which nodes we should use for training, validation, and testing."
      ],
      "metadata": {
        "id": "kswvG7Lp2Gfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, layer_name=\"GCN\", dp_rate=0.1, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of the output features. Usually number of classes in classification\n",
        "            num_layers - Number of \"hidden\" graph layers\n",
        "            layer_name - String of the graph layer to use\n",
        "            dp_rate - Dropout rate to apply throughout the network\n",
        "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        gnn_layer = gnn_layer_by_name[layer_name]\n",
        "        \n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                gnn_layer(in_channels=in_channels, \n",
        "                          out_channels=out_channels,\n",
        "                          **kwargs),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden\n",
        "        layers += [gnn_layer(in_channels=in_channels, \n",
        "                             out_channels=c_out,\n",
        "                             **kwargs)]\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "    \n",
        "    def forward(self, x, edge_index):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
        "        \"\"\"\n",
        "        for l in self.layers:\n",
        "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
        "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
        "            # we can simply check the class type.\n",
        "            if isinstance(l, geom_nn.MessagePassing):\n",
        "                x = l(x, edge_index)\n",
        "            else:\n",
        "                x = l(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "CNjjaWyTz-bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Model.    \n",
        "Good practice in node-level tasks is to create an MLP baseline that is applied to each node independently. This way we can verify whether adding the graph information to the model indeed improves the prediction, or not"
      ],
      "metadata": {
        "id": "8Ip02xcT0GRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of the output features. Usually number of classes in classification\n",
        "            num_layers - Number of hidden layers\n",
        "            dp_rate - Dropout rate to apply throughout the network\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_channels, out_channels = c_in, c_hidden\n",
        "        for l_idx in range(num_layers-1):\n",
        "            layers += [\n",
        "                nn.Linear(in_channels, out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(dp_rate)\n",
        "            ]\n",
        "            in_channels = c_hidden\n",
        "        layers += [nn.Linear(in_channels, c_out)]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "        \"\"\"\n",
        "        return self.layers(x)\n"
      ],
      "metadata": {
        "id": "BHP9RYse0D09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " merge the models into a PyTorch Lightning module which handles the training, validation, and testing"
      ],
      "metadata": {
        "id": "T9VurRnG2Rhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NodeLevelGNN(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, model_name, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        if model_name == \"MLP\":\n",
        "            self.model = MLPModel(**model_kwargs)\n",
        "        else:\n",
        "            self.model = GNNModel(**model_kwargs)\n",
        "        self.loss_module = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.model(x, edge_index)\n",
        "        \n",
        "        # Only calculate the loss on the nodes corresponding to the mask\n",
        "        if mode == \"train\":\n",
        "            mask = data.train_mask\n",
        "        elif mode == \"val\":\n",
        "            mask = data.val_mask\n",
        "        elif mode == \"test\":\n",
        "            mask = data.test_mask\n",
        "        else:\n",
        "            assert False, f\"Unknown forward mode: {mode}\"\n",
        "        \n",
        "        loss = self.loss_module(x[mask], data.y[mask])\n",
        "        acc = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # We use SGD here, but Adam works as well \n",
        "        optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ],
      "metadata": {
        "id": "1NDgciZR0IiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_node_classifier(model_name, dataset, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "    node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
        "    \n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"NodeLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
        "                         max_epochs=200,\n",
        "                         progress_bar_refresh_rate=0) # 0 because epoch size is 1\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"NodeLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = NodeLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything()\n",
        "        model = NodeLevelGNN(model_name=model_name, c_in=dataset.num_node_features, c_out=dataset.num_classes, **model_kwargs)\n",
        "        trainer.fit(model, node_data_loader, node_data_loader)\n",
        "        model = NodeLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    \n",
        "    # Test best model on the test set\n",
        "    test_result = trainer.test(model, node_data_loader, verbose=False)\n",
        "    batch = next(iter(node_data_loader))\n",
        "    batch = batch.to(model.device)\n",
        "    _, train_acc = model.forward(batch, mode=\"train\")\n",
        "    _, val_acc = model.forward(batch, mode=\"val\")\n",
        "    result = {\"train\": train_acc,\n",
        "              \"val\": val_acc,\n",
        "              \"test\": test_result[0]['test_acc']}\n",
        "    return model, result"
      ],
      "metadata": {
        "id": "rvpK5Z2g0M9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train MLP Model"
      ],
      "metadata": {
        "id": "z2AswjbP0QmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Small function for printing the test scores\n",
        "def print_results(result_dict):\n",
        "    if \"train\" in result_dict:\n",
        "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
        "    if \"val\" in result_dict:\n",
        "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
        "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
      ],
      "metadata": {
        "id": "xIMpX6u20PYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_mlp_model, node_mlp_result = train_node_classifier(model_name=\"MLP\",\n",
        "                                                        dataset=cora_dataset,\n",
        "                                                        c_hidden=16,\n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "\n",
        "print_results(node_mlp_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMBoVukZ0T4D",
        "outputId": "6b214f14-17a3-4b84-cf6c-297afbccb974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: ../saved_models/tutorial7/NodeLevelMLP/lightning_logs\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2708. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n",
            "Train accuracy: 97.14%\n",
            "Val accuracy:   54.60%\n",
            "Test accuracy:  60.60%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node_gnn_model, node_gnn_result = train_node_classifier(model_name=\"GNN\",\n",
        "                                                        layer_name=\"GCN\",\n",
        "                                                        dataset=cora_dataset, \n",
        "                                                        c_hidden=16, \n",
        "                                                        num_layers=2,\n",
        "                                                        dp_rate=0.1)\n",
        "print_results(node_gnn_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Db4H4_x40Vx8",
        "outputId": "394a21cf-a4b5-421e-fba7-ad685574323b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: ../saved_models/tutorial7/NodeLevelGNN/lightning_logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n",
            "Train accuracy: 100.00%\n",
            "Val accuracy:   78.60%\n",
            "Test accuracy:  82.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Edge Prediction Problem\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wEfBQgz50aFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tu_dataset = torch_geometric.datasets.TUDataset(root=DATASET_PATH, name=\"MUTAG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uKGTASk0Xqi",
        "outputId": "8ec3cdb2-007c-4c57-c27b-0d6c98ebaecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting ../data/MUTAG/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data object:\", tu_dataset.data)\n",
        "print(\"Length:\", len(tu_dataset))\n",
        "print(f\"Average label: {tu_dataset.data.y.float().mean().item():4.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h05Dn9VI0ds9",
        "outputId": "a2023ae6-7e7e-42c0-955e-e1cee404bc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data object: Data(x=[3371, 7], edge_index=[2, 7442], edge_attr=[7442, 4], y=[188])\n",
            "Length: 188\n",
            "Average label: 0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "tu_dataset.shuffle()\n",
        "train_dataset = tu_dataset[:150]\n",
        "test_dataset = tu_dataset[150:]"
      ],
      "metadata": {
        "id": "vIHe0NrR0gBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_train_loader = geom_data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset\n",
        "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAORD4eV0iKK",
        "outputId": "1e7aeb23-4f00-4f70-d74d-5f02b04d7f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch_geometric/deprecation.py:12: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(graph_test_loader))\n",
        "print(\"Batch:\", batch)\n",
        "print(\"Labels:\", batch.y[:10])\n",
        "print(\"Batch indices:\", batch.batch[:40])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_TiuW3y0j3w",
        "outputId": "fbb249fa-f5eb-407b-cd47-4024f72c768d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch: DataBatch(edge_index=[2, 1512], x=[687, 7], edge_attr=[1512, 4], y=[38], batch=[687], ptr=[39])\n",
            "Labels: tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 0])\n",
            "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grap GNN model"
      ],
      "metadata": {
        "id": "C6WWgcZV0rIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphGNNModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - Dimension of input features\n",
        "            c_hidden - Dimension of hidden features\n",
        "            c_out - Dimension of output features (usually number of classes)\n",
        "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
        "            kwargs - Additional arguments for the GNNModel object\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.GNN = GNNModel(c_in=c_in, \n",
        "                            c_hidden=c_hidden, \n",
        "                            c_out=c_hidden, # Not our prediction output yet!\n",
        "                            **kwargs)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Dropout(dp_rate_linear),\n",
        "            nn.Linear(c_hidden, c_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, batch_idx):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features per node\n",
        "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
        "            batch_idx - Index of batch element for each node\n",
        "        \"\"\"\n",
        "        x = self.GNN(x, edge_index)\n",
        "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
        "        x = self.head(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tAT0Kvzi0noK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphLevelGNN(pl.LightningModule):\n",
        "    \n",
        "    def __init__(self, **model_kwargs):\n",
        "        super().__init__()\n",
        "        # Saving hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.model = GraphGNNModel(**model_kwargs)\n",
        "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, data, mode=\"train\"):\n",
        "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
        "        x = self.model(x, edge_index, batch_idx)\n",
        "        x = x.squeeze(dim=-1)\n",
        "        \n",
        "        if self.hparams.c_out == 1:\n",
        "            preds = (x > 0).float()\n",
        "            data.y = data.y.float()\n",
        "        else:\n",
        "            preds = x.argmax(dim=-1)\n",
        "        loss = self.loss_module(x, data.y)\n",
        "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
        "        return loss, acc\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, acc = self.forward(batch, mode=\"train\")\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"val\")\n",
        "        self.log('val_acc', acc)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _, acc = self.forward(batch, mode=\"test\")\n",
        "        self.log('test_acc', acc)"
      ],
      "metadata": {
        "id": "RrAU2sNJ0s5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_graph_classifier(model_name, **model_kwargs):\n",
        "    pl.seed_everything(42)\n",
        "    \n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
        "                         max_epochs=500,\n",
        "                         progress_bar_refresh_rate=0)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "\n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"GraphLevel{model_name}.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        pl.seed_everything(42)\n",
        "        model = GraphLevelGNN(c_in=tu_dataset.num_node_features, \n",
        "                              c_out=1 if tu_dataset.num_classes==2 else tu_dataset.num_classes, \n",
        "                              **model_kwargs)\n",
        "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
        "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    # Test best model on validation and test set\n",
        "    train_result = trainer.test(model, graph_train_loader, verbose=False)\n",
        "    test_result = trainer.test(model, graph_test_loader, verbose=False)\n",
        "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']} \n",
        "    return model, result"
      ],
      "metadata": {
        "id": "5nZ1VWol0w7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, result = train_graph_classifier(model_name=\"GraphConv\", \n",
        "                                       c_hidden=256, \n",
        "                                       layer_name=\"GraphConv\", \n",
        "                                       num_layers=3, \n",
        "                                       dp_rate_linear=0.5,\n",
        "                                       dp_rate=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6viiK_b0zbI",
        "outputId": "22c95e88-556a-48d1-bbd7-e3d8c713f320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 42\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/callback_connector.py:97: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
            "  f\"Setting `Trainer(progress_bar_refresh_rate={progress_bar_refresh_rate})` is deprecated in v1.5 and\"\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "Missing logger folder: ../saved_models/tutorial7/GraphLevelGraphConv/lightning_logs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found pretrained model, loading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
            "  category=PossibleUserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/data.py:73: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  \"Trying to infer the `batch_size` from an ambiguous collection. The batch size we\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train performance: {100.0*result['train']:4.2f}%\")\n",
        "print(f\"Test performance:  {100.0*result['test']:4.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJNlN-SY01Q6",
        "outputId": "880a1c06-dc94-442e-ad5a-7684b45e9ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train performance: 93.28%\n",
            "Test performance:  92.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graph Classification Problem"
      ],
      "metadata": {
        "id": "MON2ISgL04Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXERmVWU03sI",
        "outputId": "5d9c7271-5330-484c-b70b-52114b01c605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph classification with Graph Neural Networks"
      ],
      "metadata": {
        "id": "QLAaMDvG1BOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')\n",
        "\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0]  # Get the first graph object.\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QpluXsG09hA",
        "outputId": "e15ba125-4665-4839-9e4d-5d2956df252f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Extracting data/TUDataset/MUTAG/MUTAG.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of graphs: 188\n",
            "Number of features: 7\n",
            "Number of classes: 2\n",
            "\n",
            "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Average node degree: 2.24\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(12345)\n",
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muVmowCR1F1A",
        "outputId": "0a4f5943-faa4-43e3-e58c-4a47981989fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini batching of graph"
      ],
      "metadata": {
        "id": "SHq5rkQA1KoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "for step, data in enumerate(train_loader):\n",
        "    print(f'Step {step + 1}:')\n",
        "    print('=======')\n",
        "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
        "    print(data)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9ow33Sm1IcI",
        "outputId": "33a2e161-548f-431d-c659-2818c9b57101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2636], x=[1188, 7], edge_attr=[2636, 4], y=[64], batch=[1188], ptr=[65])\n",
            "\n",
            "Step 2:\n",
            "=======\n",
            "Number of graphs in the current batch: 64\n",
            "DataBatch(edge_index=[2, 2506], x=[1139, 7], edge_attr=[2506, 4], y=[64], batch=[1139], ptr=[65])\n",
            "\n",
            "Step 3:\n",
            "=======\n",
            "Number of graphs in the current batch: 22\n",
            "DataBatch(edge_index=[2, 852], x=[387, 7], edge_attr=[852, 4], y=[22], batch=[387], ptr=[23])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        torch.manual_seed(12345)\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings \n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_9au3ui1Nwm",
        "outputId": "97aadb63-ba7c-4c51-fdf4-fd3dc1935f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): GCNConv(7, 64)\n",
            "  (conv2): GCNConv(64, 64)\n",
            "  (conv3): GCNConv(64, 64)\n",
            "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = GCN(hidden_channels=64)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
        "         loss = criterion(out, data.y)  # Compute the loss.\n",
        "         loss.backward()  # Derive gradients.\n",
        "         optimizer.step()  # Update parameters based on gradients.\n",
        "         optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "     model.eval()\n",
        "\n",
        "     correct = 0\n",
        "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "         out = model(data.x, data.edge_index, data.batch)  \n",
        "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
        "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
        "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 171):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "PUVvWvn71RQs",
        "outputId": "b6be3f39-2993-4bda-8c1b-07b875ba1456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 002, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 003, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 004, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 005, Train Acc: 0.6467, Test Acc: 0.7368\n",
            "Epoch: 006, Train Acc: 0.6533, Test Acc: 0.7368\n",
            "Epoch: 007, Train Acc: 0.7467, Test Acc: 0.7632\n",
            "Epoch: 008, Train Acc: 0.7267, Test Acc: 0.7632\n",
            "Epoch: 009, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 010, Train Acc: 0.7133, Test Acc: 0.7895\n",
            "Epoch: 011, Train Acc: 0.7200, Test Acc: 0.7632\n",
            "Epoch: 012, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 013, Train Acc: 0.7200, Test Acc: 0.7895\n",
            "Epoch: 014, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 015, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 016, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 017, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 018, Train Acc: 0.7133, Test Acc: 0.8421\n",
            "Epoch: 019, Train Acc: 0.7400, Test Acc: 0.7895\n",
            "Epoch: 020, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 021, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 022, Train Acc: 0.7467, Test Acc: 0.7895\n",
            "Epoch: 023, Train Acc: 0.7533, Test Acc: 0.7895\n",
            "Epoch: 024, Train Acc: 0.7267, Test Acc: 0.8421\n",
            "Epoch: 025, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 026, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 027, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 028, Train Acc: 0.7533, Test Acc: 0.8421\n",
            "Epoch: 029, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 030, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 031, Train Acc: 0.7600, Test Acc: 0.8158\n",
            "Epoch: 032, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 033, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 034, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 035, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 036, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 037, Train Acc: 0.7400, Test Acc: 0.7632\n",
            "Epoch: 038, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 039, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 040, Train Acc: 0.7533, Test Acc: 0.7368\n",
            "Epoch: 041, Train Acc: 0.7467, Test Acc: 0.7368\n",
            "Epoch: 042, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 043, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 044, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 045, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 046, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 047, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 048, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 049, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 050, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 051, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 052, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 053, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 054, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 055, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 056, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 057, Train Acc: 0.7533, Test Acc: 0.7632\n",
            "Epoch: 058, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 059, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 060, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 061, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 062, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 063, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 064, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 065, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 066, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 067, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 068, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 069, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 070, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 071, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 072, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 073, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 074, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 075, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 076, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 077, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 078, Train Acc: 0.7733, Test Acc: 0.8421\n",
            "Epoch: 079, Train Acc: 0.7667, Test Acc: 0.8158\n",
            "Epoch: 080, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 081, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 082, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 083, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 084, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 085, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 086, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 087, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 088, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 089, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 090, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 091, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 092, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 093, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 094, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 095, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 096, Train Acc: 0.7600, Test Acc: 0.7895\n",
            "Epoch: 097, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 098, Train Acc: 0.7733, Test Acc: 0.8158\n",
            "Epoch: 099, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 100, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 101, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 102, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 103, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 104, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 105, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 106, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 107, Train Acc: 0.7733, Test Acc: 0.7105\n",
            "Epoch: 108, Train Acc: 0.8000, Test Acc: 0.7632\n",
            "Epoch: 109, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 110, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 111, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 112, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 113, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 114, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 115, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 116, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 117, Train Acc: 0.7733, Test Acc: 0.7895\n",
            "Epoch: 118, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 119, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 120, Train Acc: 0.8000, Test Acc: 0.7105\n",
            "Epoch: 121, Train Acc: 0.7600, Test Acc: 0.7632\n",
            "Epoch: 122, Train Acc: 0.7667, Test Acc: 0.7105\n",
            "Epoch: 123, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 124, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 125, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 126, Train Acc: 0.7733, Test Acc: 0.7368\n",
            "Epoch: 127, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 128, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 129, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 130, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 131, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 132, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 133, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 134, Train Acc: 0.7667, Test Acc: 0.7632\n",
            "Epoch: 135, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 136, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 137, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 138, Train Acc: 0.8133, Test Acc: 0.7105\n",
            "Epoch: 139, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 140, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 141, Train Acc: 0.8000, Test Acc: 0.6579\n",
            "Epoch: 142, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 143, Train Acc: 0.7933, Test Acc: 0.7632\n",
            "Epoch: 144, Train Acc: 0.7867, Test Acc: 0.7368\n",
            "Epoch: 145, Train Acc: 0.8267, Test Acc: 0.7368\n",
            "Epoch: 146, Train Acc: 0.7667, Test Acc: 0.7895\n",
            "Epoch: 147, Train Acc: 0.7800, Test Acc: 0.7105\n",
            "Epoch: 148, Train Acc: 0.7933, Test Acc: 0.7895\n",
            "Epoch: 149, Train Acc: 0.8200, Test Acc: 0.7105\n",
            "Epoch: 150, Train Acc: 0.7800, Test Acc: 0.7895\n",
            "Epoch: 151, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 152, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 153, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 154, Train Acc: 0.8067, Test Acc: 0.7368\n",
            "Epoch: 155, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 156, Train Acc: 0.7800, Test Acc: 0.7105\n",
            "Epoch: 157, Train Acc: 0.8000, Test Acc: 0.7368\n",
            "Epoch: 158, Train Acc: 0.7800, Test Acc: 0.7368\n",
            "Epoch: 159, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 160, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 161, Train Acc: 0.7800, Test Acc: 0.7632\n",
            "Epoch: 162, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 163, Train Acc: 0.7867, Test Acc: 0.7632\n",
            "Epoch: 164, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 165, Train Acc: 0.7800, Test Acc: 0.8158\n",
            "Epoch: 166, Train Acc: 0.7733, Test Acc: 0.7632\n",
            "Epoch: 167, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 168, Train Acc: 0.7867, Test Acc: 0.7895\n",
            "Epoch: 169, Train Acc: 0.8000, Test Acc: 0.7632\n",
            "Epoch: 170, Train Acc: 0.8000, Test Acc: 0.7632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SojM6_Ol1Tx1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}